{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract DiD Tables from MySQL\n",
        "\n",
        "Extracts difference-in-differences panel and estimate tables from MySQL database `causal_analytics`.\n",
        "\n",
        "**Inputs:**\n",
        "- MySQL database connection (credentials from `.env` in repo root)\n",
        "- Schema: `causal_analytics`\n",
        "\n",
        "**Outputs:**\n",
        "- Parquet files in `data/intermediate/`\n",
        "- Manifest JSON in `results/run_manifests/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.engine import URL\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "try:\n",
        "    import pyarrow as pa\n",
        "    import pyarrow.parquet as pq\n",
        "    HAS_PYARROW = True\n",
        "except ImportError:\n",
        "    HAS_PYARROW = False\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repository Root Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 14:31:50,100 - INFO - Repository root: /Users/rajnishpanwar/Desktop/Casual Analytics\n"
          ]
        }
      ],
      "source": [
        "def find_repo_root(start: Path) -> Path:\n",
        "    \"\"\"Find repository root by locating README.md.\"\"\"\n",
        "    p = start.resolve()\n",
        "    for _ in range(12):\n",
        "        if (p / \"README.md\").exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError(\"Repository root not found (README.md missing)\")\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "logger.info(f\"Repository root: {REPO_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_path = REPO_ROOT / \".env\"\n",
        "if not env_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Environment file not found: {env_path}. \"\n",
        "        f\"Create .env in repo root with MYSQL_HOST, MYSQL_PORT, MYSQL_DB, MYSQL_USER, MYSQL_PASSWORD.\"\n",
        "    )\n",
        "\n",
        "load_dotenv(env_path)\n",
        "\n",
        "MYSQL_HOST = os.getenv(\"MYSQL_HOST\")\n",
        "MYSQL_PORT = os.getenv(\"MYSQL_PORT\", \"3306\")\n",
        "MYSQL_DB = os.getenv(\"MYSQL_DB\")\n",
        "MYSQL_USER = os.getenv(\"MYSQL_USER\")\n",
        "MYSQL_PASSWORD = os.getenv(\"MYSQL_PASSWORD\")\n",
        "\n",
        "if not all([MYSQL_HOST, MYSQL_DB, MYSQL_USER, MYSQL_PASSWORD]):\n",
        "    missing = [k for k, v in {\n",
        "        \"MYSQL_HOST\": MYSQL_HOST,\n",
        "        \"MYSQL_DB\": MYSQL_DB,\n",
        "        \"MYSQL_USER\": MYSQL_USER,\n",
        "        \"MYSQL_PASSWORD\": MYSQL_PASSWORD\n",
        "    }.items() if not v]\n",
        "    raise ValueError(f\"Missing required environment variables: {', '.join(missing)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Variable Diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MYSQL_HOST: 'localhost'\n",
            "MYSQL_PORT: 3306\n",
            "MYSQL_DB: 'causal_analytics'\n",
            "MYSQL_USER: 'root'\n",
            "MYSQL_PASSWORD set: True\n"
          ]
        }
      ],
      "source": [
        "print(f\"MYSQL_HOST: {repr(MYSQL_HOST)}\")\n",
        "print(f\"MYSQL_PORT: {MYSQL_PORT}\")\n",
        "print(f\"MYSQL_DB: {repr(MYSQL_DB)}\")\n",
        "print(f\"MYSQL_USER: {repr(MYSQL_USER)}\")\n",
        "print(f\"MYSQL_PASSWORD set: {MYSQL_PASSWORD is not None}\")\n",
        "\n",
        "if MYSQL_HOST.startswith('@'):\n",
        "    raise ValueError(\n",
        "        f\"MYSQL_HOST starts with '@': {repr(MYSQL_HOST)}. \"\n",
        "        f\"Fix {REPO_ROOT}/.env - remove leading '@' from MYSQL_HOST.\"\n",
        "    )\n",
        "\n",
        "if ' ' in MYSQL_HOST:\n",
        "    raise ValueError(\n",
        "        f\"MYSQL_HOST contains spaces: {repr(MYSQL_HOST)}. \"\n",
        "        f\"Fix {REPO_ROOT}/.env - remove spaces from MYSQL_HOST.\"\n",
        "    )\n",
        "\n",
        "try:\n",
        "    port_int = int(MYSQL_PORT)\n",
        "    if not (1 <= port_int <= 65535):\n",
        "        raise ValueError(f\"MYSQL_PORT out of range: {MYSQL_PORT}\")\n",
        "except ValueError as e:\n",
        "    raise ValueError(f\"Invalid MYSQL_PORT: {MYSQL_PORT}. Must be integer 1-65535.\") from e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Database Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 14:31:50,117 - INFO - Engine created for root@localhost:3306/causal_analytics\n"
          ]
        }
      ],
      "source": [
        "url = URL.create(\n",
        "    drivername=\"mysql+pymysql\",\n",
        "    username=MYSQL_USER,\n",
        "    password=MYSQL_PASSWORD,\n",
        "    host=MYSQL_HOST,\n",
        "    port=int(MYSQL_PORT),\n",
        "    database=MYSQL_DB\n",
        ")\n",
        "\n",
        "engine = create_engine(url, pool_pre_ping=True)\n",
        "logger.info(f\"Engine created for {MYSQL_USER}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connectivity Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 14:31:50,137 - INFO - Database connection successful\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(text(\"SELECT 1\"))\n",
        "        result.fetchone()\n",
        "    logger.info(\"Database connection successful\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Database connection failed: {e}\")\n",
        "    raise RuntimeError(\n",
        "        f\"Cannot connect to database. Check credentials in {REPO_ROOT}/.env and ensure MySQL server is running.\"\n",
        "    ) from e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "SCHEMA = \"causal_analytics\"\n",
        "\n",
        "TABLES_TO_EXTRACT = [\n",
        "    \"did_campaign_panel_purchase\",\n",
        "    \"did_event_study_weekly_purchase\",\n",
        "    \"did_campaign_estimates_purchase\",\n",
        "    \"did_campaign_estimates_purchase_valid\",\n",
        "    \"did_campaign_estimates_purchase_invalid\",\n",
        "    \"did_panel_coverage_purchase\",\n",
        "    \"did_overall_cell_means_purchase\",\n",
        "    \"did_overall_estimate_purchase\"\n",
        "]\n",
        "\n",
        "OUTPUT_DIR = REPO_ROOT / \"data\" / \"intermediate\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MANIFEST_DIR = REPO_ROOT / \"results\" / \"run_manifests\"\n",
        "MANIFEST_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 14:31:51,202 - INFO - Extracted did_campaign_panel_purchase: 122536 rows, 11 columns\n",
            "2026-01-14 14:31:51,371 - INFO - Extracted did_event_study_weekly_purchase: 27183 rows, 7 columns\n",
            "2026-01-14 14:31:51,392 - INFO - Extracted did_campaign_estimates_purchase: 1584 rows, 20 columns\n",
            "2026-01-14 14:31:51,394 - INFO - Extracted did_campaign_estimates_purchase_valid: 9 rows, 23 columns\n",
            "2026-01-14 14:31:51,414 - INFO - Extracted did_campaign_estimates_purchase_invalid: 1575 rows, 24 columns\n",
            "2026-01-14 14:31:51,416 - INFO - Extracted did_panel_coverage_purchase: 1 rows, 5 columns\n",
            "2026-01-14 14:31:51,417 - INFO - Extracted did_overall_cell_means_purchase: 4 rows, 6 columns\n",
            "2026-01-14 14:31:51,419 - INFO - Extracted did_overall_estimate_purchase: 1 rows, 10 columns\n"
          ]
        }
      ],
      "source": [
        "def extract_table(engine, schema: str, table_name: str) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Extract table from MySQL. Returns None if table does not exist.\"\"\"\n",
        "    try:\n",
        "        query = f\"SELECT * FROM `{schema}`.`{table_name}`\"\n",
        "        df = pd.read_sql(query, engine)\n",
        "        logger.info(f\"Extracted {table_name}: {len(df)} rows, {len(df.columns)} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to extract {table_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "extracted_data = {}\n",
        "missing_tables = []\n",
        "\n",
        "for table_name in TABLES_TO_EXTRACT:\n",
        "    df = extract_table(engine, SCHEMA, table_name)\n",
        "    if df is not None:\n",
        "        extracted_data[table_name] = df\n",
        "    else:\n",
        "        missing_tables.append(table_name)\n",
        "\n",
        "if missing_tables:\n",
        "    raise RuntimeError(\n",
        "        f\"Missing required tables in schema {SCHEMA}: {', '.join(missing_tables)}. \"\n",
        "        f\"Ensure SQL scripts have been executed to create these tables.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 14:31:51,447 - INFO - Panel table validation passed\n",
            "2026-01-14 14:31:51,447 - INFO - Estimates table validation passed\n",
            "2026-01-14 14:31:51,448 - INFO - Valid campaigns: 9\n",
            "2026-01-14 14:31:51,448 - INFO - Invalid campaigns: 1575\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Campaigns: 1584, Incomplete 2x2: 1569, Null lift: 1569\n",
            "Invalid reason breakdown: {'missing_2x2_cell': 1569, 'control_hh_pre_below_threshold': 4, 'treated_hh_pre_below_threshold': 2}\n"
          ]
        }
      ],
      "source": [
        "validation_results = {}\n",
        "\n",
        "# Panel table validation\n",
        "if \"did_campaign_panel_purchase\" in extracted_data:\n",
        "    df_panel = extracted_data[\"did_campaign_panel_purchase\"]\n",
        "    errors = []\n",
        "    \n",
        "    required_cols = [\"campaign_id\", \"household_id\", \"week_number\", \"treated\", \"post\"]\n",
        "    missing_cols = [col for col in required_cols if col not in df_panel.columns]\n",
        "    if missing_cols:\n",
        "        errors.append(f\"Missing required columns: {missing_cols}\")\n",
        "    \n",
        "    null_counts = {}\n",
        "    for col in required_cols:\n",
        "        if col in df_panel.columns:\n",
        "            null_count = df_panel[col].isnull().sum()\n",
        "            if null_count > 0:\n",
        "                null_counts[col] = int(null_count)\n",
        "    if null_counts:\n",
        "        errors.append(f\"Null values in required columns: {null_counts}\")\n",
        "    \n",
        "    treated_post_crosstab = pd.crosstab(df_panel[\"treated\"], df_panel[\"post\"], margins=True)\n",
        "    \n",
        "    distinct_households = {}\n",
        "    for t in [0, 1]:\n",
        "        for p in [0, 1]:\n",
        "            subset = df_panel[(df_panel[\"treated\"] == t) & (df_panel[\"post\"] == p)]\n",
        "            distinct_households[f\"treated_{t}_post_{p}\"] = subset[\"household_id\"].nunique()\n",
        "    \n",
        "    validation_results[\"did_campaign_panel_purchase\"] = {\n",
        "        \"valid\": len(errors) == 0,\n",
        "        \"errors\": errors,\n",
        "        \"summary\": {\n",
        "            \"treated_post_crosstab\": treated_post_crosstab.to_dict(),\n",
        "            \"distinct_households_by_cell\": distinct_households\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    if errors:\n",
        "        logger.warning(f\"Panel validation errors: {errors}\")\n",
        "    else:\n",
        "        logger.info(\"Panel table validation passed\")\n",
        "\n",
        "# Estimates table validation\n",
        "if \"did_campaign_estimates_purchase\" in extracted_data:\n",
        "    df_estimates = extracted_data[\"did_campaign_estimates_purchase\"]\n",
        "    errors = []\n",
        "    \n",
        "    if \"did_incomplete_2x2\" not in df_estimates.columns:\n",
        "        errors.append(\"Missing column: did_incomplete_2x2\")\n",
        "    \n",
        "    if \"campaign_id\" not in df_estimates.columns:\n",
        "        errors.append(\"Missing column: campaign_id\")\n",
        "    \n",
        "    n_campaigns = len(df_estimates)\n",
        "    n_incomplete = 0\n",
        "    n_null_lift = 0\n",
        "    \n",
        "    if \"did_incomplete_2x2\" in df_estimates.columns:\n",
        "        n_incomplete = int(df_estimates[\"did_incomplete_2x2\"].sum())\n",
        "    \n",
        "    if \"did_sales_lift\" in df_estimates.columns:\n",
        "        n_null_lift = int(df_estimates[\"did_sales_lift\"].isnull().sum())\n",
        "    \n",
        "    validation_results[\"did_campaign_estimates_purchase\"] = {\n",
        "        \"valid\": len(errors) == 0,\n",
        "        \"errors\": errors,\n",
        "        \"summary\": {\n",
        "            \"n_campaigns\": n_campaigns,\n",
        "            \"n_incomplete_2x2\": n_incomplete,\n",
        "            \"n_null_did_sales_lift\": n_null_lift\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(f\"Campaigns: {n_campaigns}, Incomplete 2x2: {n_incomplete}, Null lift: {n_null_lift}\")\n",
        "    \n",
        "    if errors:\n",
        "        logger.warning(f\"Estimates validation errors: {errors}\")\n",
        "    else:\n",
        "        logger.info(\"Estimates table validation passed\")\n",
        "\n",
        "# Valid/invalid counts\n",
        "if \"did_campaign_estimates_purchase_valid\" in extracted_data:\n",
        "    df_valid = extracted_data[\"did_campaign_estimates_purchase_valid\"]\n",
        "    n_valid = len(df_valid)\n",
        "    logger.info(f\"Valid campaigns: {n_valid}\")\n",
        "\n",
        "if \"did_campaign_estimates_purchase_invalid\" in extracted_data:\n",
        "    df_invalid = extracted_data[\"did_campaign_estimates_purchase_invalid\"]\n",
        "    n_invalid = len(df_invalid)\n",
        "    logger.info(f\"Invalid campaigns: {n_invalid}\")\n",
        "    \n",
        "    if \"invalid_reason\" in df_invalid.columns:\n",
        "        reason_counts = df_invalid[\"invalid_reason\"].value_counts().to_dict()\n",
        "        print(f\"Invalid reason breakdown: {reason_counts}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export to Parquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 14:31:51,479 - INFO - Exported did_campaign_panel_purchase to /Users/rajnishpanwar/Desktop/Casual Analytics/data/intermediate/did_campaign_panel_purchase.parquet\n",
            "2026-01-14 14:31:51,482 - INFO - Exported did_event_study_weekly_purchase to /Users/rajnishpanwar/Desktop/Casual Analytics/data/intermediate/did_event_study_weekly_purchase.parquet\n",
            "2026-01-14 14:31:51,484 - INFO - Exported did_campaign_estimates_purchase to /Users/rajnishpanwar/Desktop/Casual Analytics/data/intermediate/did_campaign_estimates_purchase.parquet\n",
            "2026-01-14 14:31:51,489 - INFO - Exported did_campaign_estimates_purchase_valid to /Users/rajnishpanwar/Desktop/Casual Analytics/data/intermediate/did_campaign_estimates_purchase_valid.parquet\n",
            "2026-01-14 14:31:51,492 - INFO - Exported did_campaign_estimates_purchase_invalid to /Users/rajnishpanwar/Desktop/Casual Analytics/data/intermediate/did_campaign_estimates_purchase_invalid.parquet\n",
            "2026-01-14 14:31:51,493 - INFO - Exported did_panel_coverage_purchase to /Users/rajnishpanwar/Desktop/Casual Analytics/data/intermediate/did_panel_coverage_purchase.parquet\n",
            "2026-01-14 14:31:51,494 - INFO - Exported did_overall_cell_means_purchase to /Users/rajnishpanwar/Desktop/Casual Analytics/data/intermediate/did_overall_cell_means_purchase.parquet\n",
            "2026-01-14 14:31:51,495 - INFO - Exported did_overall_estimate_purchase to /Users/rajnishpanwar/Desktop/Casual Analytics/data/intermediate/did_overall_estimate_purchase.parquet\n"
          ]
        }
      ],
      "source": [
        "files_written = {}\n",
        "\n",
        "for table_name, df in extracted_data.items():\n",
        "    output_path = OUTPUT_DIR / f\"{table_name}.parquet\"\n",
        "    \n",
        "    try:\n",
        "        if HAS_PYARROW:\n",
        "            df.to_parquet(output_path, index=False, engine='pyarrow')\n",
        "        else:\n",
        "            logger.warning(\"pyarrow not available, falling back to CSV\")\n",
        "            output_path = OUTPUT_DIR / f\"{table_name}.csv\"\n",
        "            df.to_csv(output_path, index=False)\n",
        "        \n",
        "        files_written[table_name] = str(output_path)\n",
        "        logger.info(f\"Exported {table_name} to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to export {table_name}: {e}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Manifest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 14:31:51,500 - INFO - Manifest written to /Users/rajnishpanwar/Desktop/Casual Analytics/results/run_manifests/extraction_manifest_20260114_143151.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Manifest: /Users/rajnishpanwar/Desktop/Casual Analytics/results/run_manifests/extraction_manifest_20260114_143151.json\n"
          ]
        }
      ],
      "source": [
        "def get_package_versions() -> Dict[str, str]:\n",
        "    \"\"\"Get versions of key packages.\"\"\"\n",
        "    versions = {}\n",
        "    for package in [\"pandas\", \"sqlalchemy\", \"pymysql\"]:\n",
        "        try:\n",
        "            mod = __import__(package)\n",
        "            versions[package] = getattr(mod, \"__version__\", \"unknown\")\n",
        "        except ImportError:\n",
        "            versions[package] = \"not_installed\"\n",
        "    if HAS_PYARROW:\n",
        "        versions[\"pyarrow\"] = pa.__version__\n",
        "    return versions\n",
        "\n",
        "\n",
        "manifest = {\n",
        "    \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "    \"python_version\": sys.version,\n",
        "    \"package_versions\": get_package_versions(),\n",
        "    \"tables_extracted\": {\n",
        "        name: {\n",
        "            \"row_count\": len(df),\n",
        "            \"column_count\": len(df.columns)\n",
        "        }\n",
        "        for name, df in extracted_data.items()\n",
        "    },\n",
        "    \"files_written\": files_written,\n",
        "    \"missing_tables\": missing_tables,\n",
        "    \"validation_results\": validation_results\n",
        "}\n",
        "\n",
        "manifest_path = MANIFEST_DIR / f\"extraction_manifest_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "with open(manifest_path, 'w') as f:\n",
        "    json.dump(manifest, f, indent=2, default=str)\n",
        "\n",
        "logger.info(f\"Manifest written to {manifest_path}\")\n",
        "print(f\"\\nManifest: {manifest_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (causal_analytics)",
      "language": "python",
      "name": "causal_analytics"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
